# Generated by Django 4.0.3 on 2022-03-27 22:32



#GENERATE TOP 1000 CODE
import numpy as np
from tqdm import tqdm

#function and var definitions
nWords = 0
vecSize = 0
embeddings = {}

def sigmoid(x):
    if x == 1:
        return 100
    return 100 * np.tanh(x)

def score(a, b):
    x = a * b
    x = sum(x) / (np.linalg.norm(a) * np.linalg.norm(b))
    x = round(x, 6)
    return sigmoid(x)

from enum import Enum
class KeyType(Enum):
    INVALID = 1
    HASHTAG = 2
    VALID = 0

def isValidKey(key): 
    if len(key) == 0:
        return KeyType.INVALID
    elif key[0] == '#':
        return KeyType.HASHTAG
    for char in key:
        if ord(char) <= 128:
            return KeyType.INVALID
    else:
        return KeyType.VALID

# read data into dict
def read_data_into_dict():
    with open('/Users/bigsad/Downloads/jawiki.all_vectors.300d.txt') as f:
    # with open('test dataset.txt') as f:
        line = f.readline()
        nWords, vecSize = line.split(' ')
        # nWords = int(nWords)
        nWords = 100000
        vecSize = int(vecSize)
        with tqdm(total=nWords) as pbar:
            count = 1
            while line:
                line = f.readline()
                line = line[:-1] # remove newline
                arr = np.array(line.split(' '))

                # first element is the japanese word and the rest are the vector values
                key = arr[0] 
                if isValidKey(key) == KeyType.INVALID:
                    pass
                elif isValidKey(key) == KeyType.HASHTAG:
                    key = key[2:-2]
                
                vec = arr[1:]
                vec = vec.astype('float64')
                embeddings[key] = vec
                
                if count == nWords:
                    break
                count += 1
                pbar.update(1)


    #clean up other miscel characters
    del embeddings['']
    del embeddings['、']
    del embeddings['。']
    del embeddings['（']
    del embeddings['）']
    del embeddings['「']
    del embeddings['」']
    del embeddings['・']

#GET TARGET WORDS
def get_target_words():
    from bs4 import BeautifulSoup
    import requests as req

    urls = [
            "https://japanesetest4you.com/jlpt-n1-vocabulary-list/",
            "https://japanesetest4you.com/jlpt-n2-vocabulary-list/",
            "https://japanesetest4you.com/jlpt-n3-vocabulary-list/",
            "https://japanesetest4you.com/jlpt-n4-vocabulary-list/",
            "https://japanesetest4you.com/jlpt-n5-vocabulary-list/",
            "https://japanesetest4you.com/jlpt-n1-grammar-list/",
            "https://japanesetest4you.com/jlpt-n2-grammar-list/",
            "https://japanesetest4you.com/jlpt-n3-grammar-list/",
            "https://japanesetest4you.com/jlpt-n4-grammar-list/",
            "https://japanesetest4you.com/jlpt-n5-grammar-list/",
          ]
    target_words = set()

    for url in urls:
        content = req.get(url).text
        soup = BeautifulSoup(content, 'lxml')
        
        for div in soup.find_all('div', class_='entry clearfix'):
            for p in tqdm(div.find_all('p')):
                word = p.text.split(' ')[0]
                if word in embeddings:
                    target_words.add(word)

    #https://kyoan.u-biq.org/tangosearch.html has a pretty comprehensive list too so I'll include that
    url = "https://kyoan.u-biq.org/tangosearch.html"
    content = req.get(url)
    content = req.get(url).text
    content = content.encode('latin1')
    soup = BeautifulSoup(content, 'lxml')

    for t in soup.find_all('table', class_='hyou'):
        for td in t.tbody.find_all('td'):
            if td.text and td.text[0] == '(':
                word = td.text.split(')')[1]
            else:
                word = td.text
            
            if word in embeddings:
                target_words.add(word)

    return target_words



#GENERATE TOP 1000s for the target words
def generate_top1000(tword):
    tvec = embeddings[tword]
    top_1000_for_target = SortedDict()
    
    for word, vec in embeddings.items():
        similarity = score(tvec, vec)
        if not top_1000_for_target or len(top_1000_for_target) < 1000:
            top_1000_for_target[similarity] = word
        elif similarity > top_1000_for_target.keys()[0]:
            del top_1000_for_target[top_1000_for_target.keys()[0]]
            top_1000_for_target[similarity] = word
    
    return top_1000_for_target




#IMPORT CODE
from django.db import migrations
from api.models import Word, Top1000

from sortedcontainers import SortedDict

def populate_db(apps, schema_editor):
    read_data_into_dict()

    Word = apps.get_model('api', 'Word')
    target_words = get_target_words()

    # count = 1
    for tword in tqdm(target_words):
        top_1000_for_target = generate_top1000(tword)
        w = Word(word_text=tword)
        w.save()
        for topWordScore, topWord in top_1000_for_target.items():
          w.top1000_set.create(top1000word=topWord, score=topWordScore)

        # if count == 10:
        #     break
        # count += 1

    

    

class Migration(migrations.Migration):

    dependencies = [
        ('api', '0001_initial'),
    ]

    operations = [
        migrations.RunPython(populate_db),
    ]
   
